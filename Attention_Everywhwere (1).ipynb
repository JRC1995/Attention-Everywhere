{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded GloVe!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from __future__ import division\n",
    "\n",
    "filename = 'glove.6B.50d.txt'\n",
    "def loadGloVe(filename):\n",
    "    vocab = []\n",
    "    embd = []\n",
    "    file = open(filename,'r')\n",
    "    for line in file.readlines():\n",
    "        row = line.strip().split(' ')\n",
    "        vocab.append(row[0])\n",
    "        embd.append(row[1:])\n",
    "    print('Loaded GloVe!')\n",
    "    file.close()\n",
    "    return vocab,embd\n",
    "vocab,embd = loadGloVe(filename)\n",
    "\n",
    "embedding = np.asarray(embd)\n",
    "embedding = embedding.astype(np.float32)\n",
    "\n",
    "word_vec_dim = len(embedding[0])\n",
    "#Pre-trained GloVe embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def np_nearest_neighbour(x):\n",
    "    #returns array in embedding that's most similar (in terms of cosine similarity) to x\n",
    "        \n",
    "    xdoty = np.multiply(embedding,x)\n",
    "    xdoty = np.sum(xdoty,1)\n",
    "    xlen = np.square(x)\n",
    "    xlen = np.sum(xlen,0)\n",
    "    xlen = np.sqrt(xlen)\n",
    "    ylen = np.square(embedding)\n",
    "    ylen = np.sum(ylen,1)\n",
    "    ylen = np.sqrt(ylen)\n",
    "    xlenylen = np.multiply(xlen,ylen)\n",
    "    cosine_similarities = np.divide(xdoty,xlenylen)\n",
    "\n",
    "    return embedding[np.argmax(cosine_similarities)]\n",
    "\n",
    "\n",
    "def word2vec(word):  # converts a given word into its vector representation\n",
    "    if word in vocab:\n",
    "        return embedding[vocab.index(word)]\n",
    "    else:\n",
    "        return embedding[vocab.index('unk')]\n",
    "\n",
    "def vec2word(vec):   # converts a given vector representation into the represented word \n",
    "    for x in xrange(0, len(embedding)):\n",
    "        if np.array_equal(embedding[x],np.asarray(vec)):\n",
    "            return vocab[x]\n",
    "    return vec2word(np_nearest_neighbour(np.asarray(vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open ('vec_summaries', 'rb') as fp:\n",
    "    vec_summaries = pickle.load(fp)\n",
    "\n",
    "with open ('vec_texts', 'rb') as fp:\n",
    "    vec_texts = pickle.load(fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open ('vocab_limit', 'rb') as fp:\n",
    "    vocab_limit = pickle.load(fp)\n",
    "\n",
    "with open ('embd_limit', 'rb') as fp:\n",
    "    embd_limit = pickle.load(fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of dataset with summary length beyond 7: 16.146% \n",
      "Percentage of dataset with text length more than 80: 40.412% \n"
     ]
    }
   ],
   "source": [
    "#DIAGNOSIS\n",
    "\n",
    "count = 0\n",
    "\n",
    "LEN = 7\n",
    "\n",
    "for summary in vec_summaries:\n",
    "    if len(summary)-1>LEN:\n",
    "        count = count + 1\n",
    "print \"Percentage of dataset with summary length beyond \"+str(LEN)+\": \"+str((count/len(vec_summaries))*100)+\"% \"\n",
    "\n",
    "count = 0\n",
    "\n",
    "LEN = 80\n",
    "\n",
    "for text in vec_texts:\n",
    "    if len(text)>LEN:\n",
    "        count = count + 1\n",
    "print \"Percentage of dataset with text length more than \"+str(LEN)+\": \"+str((count/len(vec_texts))*100)+\"% \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SUMMARY_LEN = 7\n",
    "MAX_TEXT_LEN = 80\n",
    "\n",
    "\n",
    "\n",
    "#REMOVE DATA WHOSE SUMMARIES ARE TOO BIG\n",
    "#OR WHOSE TEXT LENGTH IS TOO BIG\n",
    "\n",
    "vec_summaries_reduced = []\n",
    "vec_texts_reduced = []\n",
    "\n",
    "i = 0\n",
    "for summary in vec_summaries:\n",
    "    if len(summary)-1<=MAX_SUMMARY_LEN and len(vec_texts[i])<=MAX_TEXT_LEN:\n",
    "        vec_summaries_reduced.append(summary)\n",
    "        vec_texts_reduced.append(vec_texts[i])\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_len = int((.7)*len(vec_summaries_reduced))\n",
    "\n",
    "train_texts = vec_texts_reduced[0:train_len]\n",
    "train_summaries = vec_summaries_reduced[0:train_len]\n",
    "\n",
    "val_len = int((.15)*len(vec_summaries_reduced))\n",
    "\n",
    "val_texts = vec_texts_reduced[train_len:train_len+val_len]\n",
    "val_summaries = vec_summaries_reduced[train_len:train_len+val_len]\n",
    "\n",
    "test_texts = vec_texts_reduced[train_len+val_len:len(vec_summaries_reduced)]\n",
    "test_summaries = vec_summaries_reduced[train_len+val_len:len(vec_summaries_reduced)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18858\n"
     ]
    }
   ],
   "source": [
    "print train_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def transform_out(output_text):\n",
    "    output_len = len(output_text)\n",
    "    transformed_output = np.zeros([output_len],dtype=np.int32)\n",
    "    for i in xrange(0,output_len):\n",
    "        transformed_output[i] = vocab_limit.index(vec2word(output_text[i]))\n",
    "    #transformed_output[output_len:MAX_LEN] = vocab_limit.index('<PAD>')\n",
    "    return transformed_output   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Some MORE hyperparameters and other stuffs\n",
    "\n",
    "hidden_size = 250\n",
    "learning_rate = 0.003\n",
    "vocab_len = len(vocab_limit)\n",
    "training_iters = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#placeholders\n",
    "tf_text = tf.placeholder(tf.float32, [None,word_vec_dim])\n",
    "tf_seq_len = tf.placeholder(tf.int32)\n",
    "tf_summary = tf.placeholder(tf.int32,[None])\n",
    "tf_output_len = tf.placeholder(tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(hs,ht,seq_len):\n",
    "    return tf.reshape(tf.matmul(hs,tf.transpose(ht)),[seq_len])\n",
    "\n",
    "\n",
    "def align(hs,ht,seq_len):\n",
    "\n",
    "    G = tf.nn.softmax(score(hs,ht,seq_len))\n",
    "    G = tf.reshape(G,[seq_len,1])\n",
    "    \n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_encoder(x,seq_len,inp_dim):\n",
    "    \n",
    "    #PARAMETERS\n",
    "    \n",
    "    Wxh = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    #Whh = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    Whh = tf.get_variable(name=\"whhf\",shape=[hidden_size,hidden_size],dtype=tf.float32,initializer=tf.orthogonal_initializer())\n",
    "    B = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    \n",
    "    Wc = tf.Variable(tf.truncated_normal(shape=[2*hidden_size,hidden_size],stddev=0.01))\n",
    "    \n",
    "    #CONSTANTS AND ARRAYS\n",
    "    hidden = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    hidden_list = tf.TensorArray(size=1,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n",
    "    forward_list = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    context_vector = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    \n",
    "    #some initial operations\n",
    "    i = 0\n",
    "    inp = tf.reshape(x[i],[1,inp_dim])\n",
    "    inp_comp = tf.matmul(inp,Wxh)\n",
    "    \n",
    "    candidate_hidden = tf.nn.elu(inp_comp + tf.matmul(hidden,Whh) + B)\n",
    "                                 \n",
    "    attended_hidden = tf.tanh(tf.matmul(tf.concat([context_vector,candidate_hidden],1),Wc))\n",
    "                                 \n",
    "    hidden = tf.nn.elu(inp_comp + tf.matmul(attended_hidden,Whh) + B)\n",
    "    \n",
    "    hidden_list = hidden_list.write(i,tf.reshape(hidden,[hidden_size]))\n",
    "    forward_list = forward_list.write(i,tf.reshape(hidden,[hidden_size]))\n",
    "    \n",
    "    i = 1\n",
    "    \n",
    "    def cond(i,hidden,hidden_list,forward_list):\n",
    "        return i<seq_len\n",
    "    \n",
    "    def body(i,hidden,hidden_list,forward_list):\n",
    "        \n",
    "        hidden_list_stack = hidden_list.stack()\n",
    "        \n",
    "        inp = tf.reshape(x[i],[1,inp_dim])\n",
    "        inp_comp = tf.matmul(inp,Wxh)\n",
    "        \n",
    "        candidate_hidden = tf.nn.elu(inp_comp + tf.matmul(hidden,Whh) + B)\n",
    "        \n",
    "        G = align(hidden_list_stack,candidate_hidden,i)\n",
    "        \n",
    "        weighted_prev_hiddens = tf.multiply(hidden_list_stack,G)\n",
    "        context_vector = tf.reduce_sum(weighted_prev_hiddens,0)\n",
    "        context_vector = tf.reshape(context_vector,[1,hidden_size])\n",
    "        \n",
    "        attended_hidden = tf.tanh(tf.matmul(tf.concat([context_vector,candidate_hidden],1),Wc))\n",
    "        \n",
    "        hidden = tf.nn.elu(inp_comp + tf.matmul(attended_hidden,Whh) + B)\n",
    "\n",
    "        hidden_list = tf.cond(i<seq_len-1,\n",
    "                              lambda: hidden_list.write(i,tf.reshape(hidden,[hidden_size])),\n",
    "                              lambda: hidden_list)\n",
    "        forward_list = forward_list.write(i,tf.reshape(hidden,[hidden_size]))\n",
    "        \n",
    "        return i+1,hidden,hidden_list,forward_list\n",
    "    \n",
    "    _,_,hidden_list,forward_list = tf.while_loop(cond,body,[i,hidden,hidden_list,forward_list])\n",
    "    \n",
    "    hidden_list.close().mark_used()\n",
    "    \n",
    "    return forward_list.stack()\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def backward_encoder(x,seq_len,inp_dim):\n",
    "    \n",
    "    #PARAMETERS\n",
    "    \n",
    "    Wxh = tf.Variable(tf.truncated_normal(shape=[word_vec_dim,hidden_size],stddev=0.01))\n",
    "    Whh = tf.get_variable(name=\"whhb\",shape=[hidden_size,hidden_size],dtype=tf.float32,initializer = tf.orthogonal_initializer())\n",
    "    Whh = tf.Variable(np.eye(hidden_size),dtype=tf.float32)\n",
    "    B = tf.Variable(tf.zeros([1,hidden_size]),dtype=tf.float32)\n",
    "    \n",
    "    Wc = tf.Variable(tf.truncated_normal(shape=[2*hidden_size,hidden_size],stddev=0.01))\n",
    "    \n",
    "    #CONSTANTS AND ARRAYS\n",
    "    hidden = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    hidden_list = tf.TensorArray(size=1,dynamic_size=True,dtype=tf.float32,clear_after_read=False)\n",
    "    hidden_list_ordered = tf.TensorArray(size=seq_len,dtype=tf.float32)\n",
    "    context_vector = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    \n",
    "    #some initial operations\n",
    "    i = seq_len-1\n",
    "    j = 0\n",
    "    inp = tf.reshape(x[i],[1,inp_dim])\n",
    "    inp_comp = tf.matmul(inp,Wxh)\n",
    "    \n",
    "    candidate_hidden = tf.nn.elu(inp_comp + tf.matmul(hidden,Whh) + B)\n",
    "                                 \n",
    "    attended_hidden = tf.tanh(tf.matmul(tf.concat([context_vector,candidate_hidden],1),Wc))\n",
    "                                 \n",
    "    hidden = tf.nn.elu(inp_comp + tf.matmul(attended_hidden,Whh) + B)\n",
    "    \n",
    "    hidden_list = hidden_list.write(j,tf.reshape(hidden,[hidden_size]))\n",
    "    hidden_list_ordered = hidden_list_ordered.write(i,tf.reshape(hidden,[hidden_size]))\n",
    "    \n",
    "    i = seq_len-2\n",
    "    j = 1\n",
    "    \n",
    "    def cond(i,j,hidden,hidden_list,hidden_list_ordered):\n",
    "        return i>-1\n",
    "    \n",
    "    def body(i,j,hidden,hidden_list,hidden_list_ordered):\n",
    "        \n",
    "        hidden_list_stack = hidden_list.stack()\n",
    "        \n",
    "        inp = tf.reshape(x[i],[1,inp_dim])\n",
    "        inp_comp = tf.matmul(inp,Wxh)\n",
    "        \n",
    "        candidate_hidden = tf.nn.elu(inp_comp + tf.matmul(hidden,Whh) + B)\n",
    "        \n",
    "        G = align(hidden_list_stack,candidate_hidden,j)\n",
    "        \n",
    "        weighted_prev_hiddens = tf.multiply(hidden_list_stack,G)\n",
    "        context_vector = tf.reduce_sum(weighted_prev_hiddens,0)\n",
    "        context_vector = tf.reshape(context_vector,[1,hidden_size])\n",
    "        \n",
    "        attended_hidden = tf.tanh(tf.matmul(tf.concat([context_vector,candidate_hidden],1),Wc))\n",
    "        \n",
    "        hidden = tf.nn.elu(inp_comp + tf.matmul(attended_hidden,Whh) + B)\n",
    "\n",
    "        hidden_list = tf.cond(j<seq_len-1,\n",
    "                              lambda: hidden_list.write(j,tf.reshape(hidden,[hidden_size])),\n",
    "                              lambda: hidden_list)\n",
    "        hidden_list_ordered = hidden_list_ordered.write(i,tf.reshape(hidden,[hidden_size]))\n",
    "        \n",
    "        return i-1,j+1,hidden,hidden_list,hidden_list_ordered\n",
    "    \n",
    "    _,_,_,hidden_list,hidden_list_ordered = tf.while_loop(cond,body,[i,j,hidden,hidden_list,hidden_list_ordered])\n",
    "\n",
    "    hidden_list.close().mark_used()\n",
    "    \n",
    "    return hidden_list_ordered.stack()\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoder(hidden_size,encoded_hidden,tf_seq_len,tf_output_len):\n",
    "    \n",
    "    #PARAMETERS\n",
    "    \n",
    "    Wyh = tf.Variable(tf.truncated_normal(shape=[vocab_len,hidden_size],stddev=0.01))\n",
    "    Whh = Whh = tf.get_variable(name=\"whho\",shape=[hidden_size,hidden_size],dtype=tf.float32,initializer = tf.orthogonal_initializer())\n",
    "    B = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    Wc = tf.Variable(tf.truncated_normal(shape=[2*hidden_size,hidden_size],stddev=0.01))\n",
    "    Wcl = tf.Variable(tf.truncated_normal(shape=[2*hidden_size,hidden_size],stddev=0.01))\n",
    "    Ws = tf.Variable(tf.truncated_normal(shape=[hidden_size,vocab_len],stddev=0.01))\n",
    "    \n",
    "    #other non-trainable values\n",
    "    hidden = encoded_hidden[0]\n",
    "    hidden = tf.reshape(hidden,[1,hidden_size])\n",
    "    \n",
    "    hidden_list_d = tf.TensorArray(dtype=tf.float32,size=1,dynamic_size=True,clear_after_read=False)\n",
    "    output = tf.TensorArray(size=tf_output_len,dtype=tf.float32)\n",
    "    \n",
    "    decoder_context_vector = tf.zeros([1,hidden_size],dtype=tf.float32)\n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    G = align(encoded_hidden,hidden,tf_seq_len)\n",
    "        \n",
    "    weighted_encoded_hiddens = tf.multiply(encoded_hidden,G)\n",
    "    encoder_context_vector = tf.reduce_sum(weighted_encoded_hiddens,0)\n",
    "    encoder_context_vector = tf.reshape(encoder_context_vector,[1,hidden_size])\n",
    "        \n",
    "    layer_attended_hidden = tf.tanh(tf.matmul(tf.concat([encoder_context_vector,hidden],1),Wcl))\n",
    "    \n",
    "    y = tf.matmul(layer_attended_hidden,Ws)\n",
    "    output = output.write(i,tf.reshape(y,[vocab_len]))\n",
    "    y = tf.nn.softmax(y)\n",
    "    \n",
    "    candidate_hidden = tf.nn.elu(tf.matmul(y,Wyh) + tf.matmul(layer_attended_hidden,Whh) + B)\n",
    "                                 \n",
    "    attended_hidden = tf.tanh(tf.matmul(tf.concat([decoder_context_vector,candidate_hidden],1),Wc))\n",
    "    \n",
    "    hidden = tf.nn.elu(tf.matmul(y,Wyh) + tf.matmul(attended_hidden,Whh) + B)\n",
    "    \n",
    "    hidden_list_d = tf.cond((i+1)<tf_output_len,\n",
    "                            lambda:hidden_list_d.write(i,tf.reshape(hidden,[hidden_size])),\n",
    "                            lambda:hidden_list_d)\n",
    "    \n",
    "    i = 1\n",
    "    \n",
    "    def cond(i,hidden,hidden_list_d,output):\n",
    "        return i < tf_output_len\n",
    "    def body(i,hidden,hidden_list_d,output):\n",
    "        \n",
    "        G = align(encoded_hidden,hidden,tf_seq_len)\n",
    "        weighted_encoded_hiddens = tf.multiply(encoded_hidden,G)\n",
    "        encoder_context_vector = tf.reduce_sum(weighted_encoded_hiddens,0)\n",
    "        encoder_context_vector = tf.reshape(encoder_context_vector,[1,hidden_size])\n",
    "        layer_attended_hidden = tf.tanh(tf.matmul(tf.concat([encoder_context_vector,hidden],1),Wcl))\n",
    "        \n",
    "        y = tf.matmul(layer_attended_hidden,Ws)\n",
    "        output = output.write(i,tf.reshape(y,[vocab_len]))\n",
    "        y = tf.nn.softmax(y)\n",
    "        \n",
    "        candidate_hidden = tf.nn.elu(tf.matmul(y,Wyh) + tf.matmul(layer_attended_hidden,Whh) + B)\n",
    "        \n",
    "        hidden_list_stack = hidden_list_d.stack()\n",
    "        \n",
    "        G_dec = align(hidden_list_stack,candidate_hidden,i)\n",
    "        \n",
    "        weighted_prev_hiddens = tf.multiply(hidden_list_stack,G_dec)\n",
    "        decoder_context_vector = tf.reduce_sum(weighted_prev_hiddens,0)\n",
    "        decoder_context_vector = tf.reshape(decoder_context_vector,[1,hidden_size])\n",
    "        \n",
    "        attended_hidden = tf.tanh(tf.matmul(tf.concat([decoder_context_vector,candidate_hidden],1),Wc))\n",
    "        \n",
    "        hidden = tf.nn.elu(tf.matmul(y,Wyh) + tf.matmul(attended_hidden,Whh) + B)\n",
    "        \n",
    "        hidden_list_d = tf.cond(i<tf_output_len-1,\n",
    "                              lambda:hidden_list_d.write(i,tf.reshape(hidden,[hidden_size])),\n",
    "                              lambda:hidden_list_d)                         \n",
    "        \n",
    "        \n",
    "        return i+1,hidden,hidden_list_d,output\n",
    "    \n",
    "    _,_,hidden_list_d,output = tf.while_loop(cond,body,[i,hidden,hidden_list_d,output])\n",
    "    \n",
    "    #hidden_list_stack_write_all = hidden_list_d.stack()\n",
    "    hidden_list_d.close().mark_used()\n",
    "    \n",
    "    return output.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(tf_text,tf_seq_len,tf_output_len):\n",
    "                               \n",
    "    forward_hidden = forward_encoder(tf_text,\n",
    "                                     tf_seq_len,\n",
    "                                     word_vec_dim)\n",
    "    \n",
    "    backward_hidden = backward_encoder(tf_text,\n",
    "                                       tf_seq_len,\n",
    "                                       word_vec_dim)\n",
    "    \n",
    "    encoded_hidden = tf.concat([forward_hidden,backward_hidden],1)\n",
    "    \n",
    "    output = decoder(2*hidden_size,\n",
    "                    encoded_hidden,\n",
    "                    tf_seq_len,\n",
    "                    tf_output_len)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(tf_text,tf_seq_len,tf_output_len)\n",
    "\n",
    "#OPTIMIZER\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=output, labels=tf_summary))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "#PREDICTION\n",
    "\n",
    "pred = tf.TensorArray(size=tf_output_len,dtype=tf.int32)\n",
    "\n",
    "i=0\n",
    "\n",
    "def cond_pred(i,pred):\n",
    "    return i<tf_output_len\n",
    "def body_pred(i,pred):\n",
    "    pred = pred.write(i,tf.cast(tf.argmax(output[i]),tf.int32))\n",
    "    return i+1,pred\n",
    "\n",
    "i,pred = tf.while_loop(cond_pred,body_pred,[i,pred]) \n",
    "\n",
    "prediction = pred.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration: 0\n",
      "Training input sequence length: 51\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "i have bought several of the vitality canned dog food products and have found them all to be of good quality. the product looks more like a stew than a processed meat and it smells better. my labrador is finicky and she appreciates this product better than most.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "yellow-brown cons cons cons\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "good quality dog food\n",
      "\n",
      "loss=10.3813\n",
      "\n",
      "Iteration: 1\n",
      "Training input sequence length: 37\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "product arrived labeled as jumbo salted peanuts ... the peanuts were actually small sized unsalted. not sure if this was an error or if the vendor intended to represent the product as `` jumbo ''.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "good food food\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "not as advertised\n",
      "\n",
      "loss=10.3795\n",
      "\n",
      "Iteration: 2\n",
      "Training input sequence length: 46\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "if you are looking for the secret ingredient in robitussin i believe i have found it. i got this in addition to the root beer extract i ordered( which was good) and made some cherry soda. the flavor is very medicinal.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "not as\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "cough medicine\n",
      "\n",
      "loss=10.4467\n",
      "\n",
      "Iteration: 3\n",
      "Training input sequence length: 32\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "great taffy at a great price. there was a wide assortment of yummy taffy. delivery was very quick. if your a taffy lover, this is a deal.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "cough cottage\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great taffy\n",
      "\n",
      "loss=10.4148\n",
      "\n",
      "Iteration: 4\n",
      "Training input sequence length: 30\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "this taffy is so good. it is very soft and chewy. the flavors are amazing. i would definitely recommend you buying it. very satisfying!!\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "taffy taffy taffy great\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "wonderful, tasty taffy\n",
      "\n",
      "loss=10.3915\n",
      "\n",
      "Iteration: 5\n",
      "Training input sequence length: 29\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "right now i 'm mostly just sprouting this so my cats can eat the grass. they love it. i rotate it around with wheatgrass and rye too\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "cough medicine\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "yay barley\n",
      "\n",
      "loss=10.471\n",
      "\n",
      "Iteration: 6\n",
      "Training input sequence length: 29\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "this is a very healthy dog food. good for their digestion. also good for small puppies. my dog eats her required amount at every feeding.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "cough not not\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "healthy dog food\n",
      "\n",
      "loss=9.05239\n",
      "\n",
      "Iteration: 7\n",
      "Training input sequence length: 19\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "good flavor! these came securely packed ... they were fresh and delicious! i love these twizzlers!\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "not not not not\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "fresh and greasy!\n",
      "\n",
      "loss=10.4117\n",
      "\n",
      "Iteration: 8\n",
      "Training input sequence length: 24\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "the strawberry twizzlers are my guilty pleasure- yummy. six pounds will be around for a while with my son and i.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "not not not not\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "strawberry twizzlers- yummy\n",
      "\n",
      "loss=10.4865\n",
      "\n",
      "Iteration: 9\n",
      "Training input sequence length: 45\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "i love eating them and they are good for watching tv and looking at movies! it is not too sweet. i like to transfer them to a zip lock baggie so they stay fresh so i can take my time eating them.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "dog dog\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "poor taste\n",
      "\n",
      "loss=10.4583\n",
      "\n",
      "Iteration: 10\n",
      "Training input sequence length: 28\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "i am very satisfied with my unk purchase. i shared these with others and we have all enjoyed them. i will definitely be ordering more.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "dog dog dog\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "love it!\n",
      "\n",
      "loss=9.20203\n",
      "\n",
      "Iteration: 11\n",
      "Training input sequence length: 31\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "candy was delivered very fast and was purchased at a reasonable price. i was home bound and unable to get to a store so this was perfect for me.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "dog dog dog\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "home delivered unk\n",
      "\n",
      "loss=11.0419\n",
      "\n",
      "Iteration: 12\n",
      "Training input sequence length: 52\n",
      "Training target outputs sequence length: 2\n",
      "\n",
      "TEXT:\n",
      "my husband is a twizzlers addict. we 've bought these many times from amazon because we 're government employees living overseas and ca n't get them in the country we are assigned to. they 've always been fresh and tasty, packed well and arrive in a timely manner.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "dog dog\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "always fresh\n",
      "\n",
      "loss=7.8186\n",
      "\n",
      "Iteration: 13\n",
      "Training input sequence length: 68\n",
      "Training target outputs sequence length: 1\n",
      "\n",
      "TEXT:\n",
      "i bought these for my husband who is currently overseas. he loves these, and apparently his staff likes them unk< br/> there are generous amounts of twizzlers in each 16-ounce bag, and this was well worth the price.< a unk '' http: unk ''> twizzlers, strawberry, 16-ounce bags( pack of 6)< unk>\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "rural\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "twizzlers\n",
      "\n",
      "loss=14.3936\n",
      "\n",
      "Iteration: 14\n",
      "Training input sequence length: 31\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "i can remember buying this candy as a kid and the quality has n't dropped in all these years. still a superb product you wo n't be disappointed with.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "fresh food!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "delicious product!\n",
      "\n",
      "loss=8.18607\n",
      "\n",
      "Iteration: 15\n",
      "Training input sequence length: 21\n",
      "Training target outputs sequence length: 1\n",
      "\n",
      "TEXT:\n",
      "i love this candy. after weight watchers i had to cut back but still have a craving for it.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "great\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "twizzlers\n",
      "\n",
      "loss=5.17898\n",
      "\n",
      "Iteration: 16\n",
      "Training input sequence length: 72\n",
      "Training target outputs sequence length: 7\n",
      "\n",
      "TEXT:\n",
      "i have lived out of the us for over 7 yrs now, and i so miss my twizzlers!! when i go back to visit or someone visits me, i always stock up. all i can say is yum!< br/> sell these in mexico and you will have a faithful buyer, more often than i 'm able to buy them right now.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "fresh!!!!!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "please sell these in mexico!!\n",
      "\n",
      "loss=9.83768\n",
      "\n",
      "Iteration: 17\n",
      "Training input sequence length: 36\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "product received is as unk< br/>< br/>< a unk '' http: unk ''> twizzlers, strawberry, 16-ounce bags( pack of 6)< unk>\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "taffy explicit explicit\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "twizzlers- strawberry\n",
      "\n",
      "loss=15.8727\n",
      "\n",
      "Iteration: 18\n",
      "Training input sequence length: 20\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "the candy is just red, no flavor. just plan and chewy. i would never buy them again\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "!!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "nasty no flavor\n",
      "\n",
      "loss=13.7712\n",
      "\n",
      "Iteration: 19\n",
      "Training input sequence length: 43\n",
      "Training target outputs sequence length: 5\n",
      "\n",
      "TEXT:\n",
      "i was so glad amazon carried these batteries. i have a hard time finding them elsewhere because they are such a unique size. i need them for my garage door unk< br/> great deal for the price.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "!!!!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "great bargain for the price\n",
      "\n",
      "loss=13.0412\n",
      "\n",
      "Iteration: 20\n",
      "Training input sequence length: 26\n",
      "Training target outputs sequence length: 5\n",
      "\n",
      "TEXT:\n",
      "this offer is a great price and a great taste, thanks amazon for selling this unk< br/>< br/> unk\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "!!!!!\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "this is my taste ...\n",
      "\n",
      "loss=11.0834\n",
      "\n",
      "Iteration: 21\n",
      "Training input sequence length: 60\n",
      "Training target outputs sequence length: 7\n",
      "\n",
      "TEXT:\n",
      "for those of us with celiac disease this product is a lifesaver and what could be better than getting it at almost half the price of the grocery or health food store! i love mccann 's instant oatmeal- all flavors!!!< br/>< br/> thanks,< br/> abby\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "taffy taffy taffy taffy taffy taffy taffy\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "love gluten free oatmeal!!!\n",
      "\n",
      "loss=12.5351\n",
      "\n",
      "Iteration: 22\n",
      "Training input sequence length: 59\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "what else do you need to know? oatmeal, instant( make it with a half cup of low-fat milk and add raisins; nuke for 90 seconds). more expensive than kroger store brand oatmeal and maybe a little tastier or better texture or something. it 's still just oatmeal. mmm, convenient!\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "poor taste taste\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 's oatmeal\n",
      "\n",
      "loss=9.35884\n",
      "\n",
      "Iteration: 23\n",
      "Training input sequence length: 79\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "i ordered this for my wife as it was unk by our daughter. she has this almost every morning and likes all flavors. she 's happy, i 'm happy!!!< br/>< a unk '' http: unk ''> mccann 's instant irish oatmeal, variety pack of regular, apples& cinnamon, and maple& brown sugar, 10-count boxes( pack of 6)< unk>\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers taffy taffy taffy\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "wife 's favorite breakfast\n",
      "\n",
      "loss=10.3175\n",
      "\n",
      "Iteration: 24\n",
      "Training input sequence length: 38\n",
      "Training target outputs sequence length: 1\n",
      "\n",
      "TEXT:\n",
      "i have mccann 's oatmeal every morning and by ordering it from amazon i am able to save almost$ 3.00 per unk< br/> it is a great product. tastes great and very healthy\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "twizzlers\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "unk\n",
      "\n",
      "loss=8.10648\n",
      "\n",
      "Iteration: 25\n",
      "Training input sequence length: 41\n",
      "Training target outputs sequence length: 3\n",
      "\n",
      "TEXT:\n",
      "mccann 's oatmeal is a good quality choice. our favorite is the apples and cinnamon, but we find that none of these are overly sugary. for a good hot breakfast in 2 minutes, this is excellent.\n",
      "\n",
      "\n",
      "PREDICTED SUMMARY:\n",
      "\n",
      "it twizzlers it\n",
      "\n",
      "ACTUAL SUMMARY:\n",
      "\n",
      "good hot breakfast\n",
      "\n",
      "loss=9.80697\n",
      "\n",
      "Iteration: 26\n",
      "Training input sequence length: 55\n",
      "Training target outputs sequence length: 4\n",
      "\n",
      "TEXT:\n",
      "we really like the mccann 's steel cut oats but find we do n't cook it up too unk< br/> this tastes much better to me than the grocery store brands and is just as"
     ]
    }
   ],
   "source": [
    "import string\n",
    "from __future__ import print_function\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "with tf.Session() as sess: # Start Tensorflow Session\n",
    "    \n",
    "    saver = tf.train.Saver() \n",
    "    # Prepares variable for saving the model\n",
    "    sess.run(init) #initialize all variables\n",
    "    step = 0   \n",
    "    loss_list=[]\n",
    "    acc_list=[]\n",
    "    val_loss_list=[]\n",
    "    val_acc_list=[]\n",
    "    best_val_acc=0\n",
    "    display_step = 1\n",
    "    \n",
    "    while step < training_iters:\n",
    "        \n",
    "        total_loss=0\n",
    "        total_acc=0\n",
    "        total_val_loss = 0\n",
    "        total_val_acc = 0\n",
    "           \n",
    "        for i in xrange(0,train_len):\n",
    "            \n",
    "            train_out = transform_out(train_summaries[i][0:len(train_summaries[i])-1])\n",
    "            \n",
    "            if i%display_step==0:\n",
    "                print(\"\\nIteration: \"+str(i))\n",
    "                print(\"Training input sequence length: \"+str(len(train_texts[i])))\n",
    "                print(\"Training target outputs sequence length: \"+str(len(train_out)))\n",
    "            \n",
    "                print(\"\\nTEXT:\")\n",
    "                flag = 0\n",
    "                for vec in train_texts[i]:\n",
    "                    if vec2word(vec) in string.punctuation or flag==0:\n",
    "                        print(str(vec2word(vec)),end='')\n",
    "                    else:\n",
    "                        print((\" \"+str(vec2word(vec))),end='')\n",
    "                    flag=1\n",
    "\n",
    "                print(\"\\n\")\n",
    "\n",
    "\n",
    "            # Run optimization operation (backpropagation)\n",
    "            _,loss,pred = sess.run([optimizer,cost,prediction],feed_dict={tf_text: train_texts[i], \n",
    "                                                    tf_seq_len: len(train_texts[i]), \n",
    "                                                    tf_summary: train_out,\n",
    "                                                    tf_output_len: len(train_out)})\n",
    "            \n",
    "         \n",
    "            if i%display_step==0:\n",
    "                print(\"\\nPREDICTED SUMMARY:\\n\")\n",
    "                flag = 0\n",
    "                for index in pred:\n",
    "                    #if int(index)!=vocab_limit.index('eos'):\n",
    "                    if vocab_limit[int(index)] in string.punctuation or flag==0:\n",
    "                        print(str(vocab_limit[int(index)]),end='')\n",
    "                    else:\n",
    "                        print(\" \"+str(vocab_limit[int(index)]),end='')\n",
    "                    flag=1\n",
    "                print(\"\\n\")\n",
    "                \n",
    "                print(\"ACTUAL SUMMARY:\\n\")\n",
    "                flag = 0\n",
    "                for vec in train_summaries[i]:\n",
    "                    if vec2word(vec)!='eos':\n",
    "                        if vec2word(vec) in string.punctuation or flag==0:\n",
    "                            print(str(vec2word(vec)),end='')\n",
    "                        else:\n",
    "                            print((\" \"+str(vec2word(vec))),end='')\n",
    "                    flag=1\n",
    "\n",
    "                print(\"\\n\")\n",
    "            \n",
    "                #print(hs)\n",
    "            \n",
    "                print(\"loss=\"+str(loss))\n",
    "            \n",
    "            #print(h)\n",
    "            #print(out)\n",
    "            #print(ht_s)\n",
    "            \n",
    "        step=step+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
