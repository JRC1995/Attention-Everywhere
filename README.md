# Attention-Everywhere
An experimental custom seq-2-seq model with both layer-wise and temporal (recurrent) attention (attention to previous hidden states of the same RNN unit)
